{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RAW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir: str = \"raw_data/\", verbose: bool = False) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    c = 0\n",
    "    for file in glob(data_dir + \"Reviews-*.parquet\"):\n",
    "        if verbose:\n",
    "            print(f\"Reading in:   {file}\")\n",
    "        dfs.append(pd.read_parquet(file))\n",
    "        c += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Loaded {c} files\")\n",
    "\n",
    "    concat_df = pd.concat(dfs).drop_duplicates(keep=\"first\")\n",
    "    return concat_df.reset_index().iloc[:, 1:]\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform / Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text: str):\n",
    "    \"\"\"Removes first character if it is a whitespace\"\"\"\n",
    "    if text[0] == \" \":\n",
    "        return text[1:]\n",
    "    return text\n",
    "\n",
    "\n",
    "def transform_job_titles(job_title: str):\n",
    "    \"\"\"Puts a whitespace if non between rank and title\"\"\"\n",
    "    if job_title.endswith(\"I\") or job_title.endswith(\"V\"):\n",
    "        l = len(job_title)\n",
    "        for i, c in enumerate(reversed(job_title)):\n",
    "            if c not in [\"V\", \"I\", \" \"]:\n",
    "                return job_title[0 : l - i] + \" \" + job_title[l - i :]\n",
    "    else:\n",
    "        return job_title\n",
    "    \n",
    "def transform_status(status: str):\n",
    "    status_mapping = {\n",
    "        \"KEY NOT FOUND: jobLine.seasonal-current\": \"Current Seasonal\",\n",
    "        \"KEY NOT FOUND: jobLine.seasonal-former\": \"Former Seasonal\",\n",
    "        \"KEY NOT FOUND: jobLine.self_employ-former\": \"Former Self Employed\",\n",
    "        \"KEY NOT FOUND: jobLine.self_employ-current\": \"Current Self Employed\",\n",
    "        \"KEY NOT FOUND: jobLine.reserve-current\": \"Current Reserve\",\n",
    "        \"KEY NOT FOUND: jobLine.reserve-former\": \"Former Reserve\",\n",
    "        \"KEY NOT FOUND: jobLine.per_diem-current\": \"Current Per Diem\",\n",
    "        \"KEY NOT FOUND: jobLine.per_diem-former\": \"Former Per Diem\",\n",
    "    }\n",
    "    if status in status_mapping.keys():\n",
    "        return status_mapping[status]\n",
    "    else:\n",
    "        return status\n",
    "\n",
    "\n",
    "def transform_helpful_column(text: str):\n",
    "    \"\"\"Transforms the helpful column to a number\"\"\"\n",
    "    text = text.replace(\"\\xa0\", \"\")\n",
    "    if text == \"Be the first to find this review helpful\":\n",
    "        return 0\n",
    "    elif \"people\" in text:\n",
    "        return int(text.replace(\"people found this review helpful\", \"\"))\n",
    "    elif \"person\" in text:\n",
    "        return int(text.replace(\"person found this review helpful\", \"\"))\n",
    "\n",
    "# Transforming the data\n",
    "df = df.assign(\n",
    "    rating=pd.to_numeric(df[\"rating\"]).astype(np.int8),\n",
    "    status=df[\"status\"]\n",
    "    .apply(lambda x: transform_status(x))\n",
    "    .astype(\"category\"),\n",
    "    date=pd.to_datetime(df[\"date\"]),\n",
    "    experience=df[\"experience\"].astype(\"category\"),\n",
    "    company=df[\"company\"].astype(\"category\"),\n",
    "    category=df[\"category\"].astype(\"category\"),\n",
    "    helpful=df[\"helpful\"].apply(lambda x: transform_helpful_column(x)).astype(np.int16),\n",
    "    job_title=df[\"job_title\"]\n",
    "    .apply(lambda x: x.replace(\"\\xa0\", \" \"))\n",
    "    .apply(lambda x: remove_whitespace(x))\n",
    "    .apply(lambda x: transform_job_titles(x)),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Rank Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_ranks = [\"i\", ]\n",
    "def extract_job_title_rank(job_title: str):\n",
    "    \"\"\"Extracts the rank from the job title\"\"\"\n",
    "    job_ranks = [\"Senior\", \"Sr.\", \"Junior\", \"Jr.\", \"sr\"]\n",
    "\n",
    "    junior =  [\"i\"]\n",
    "    #junior to lowercase\n",
    "    junior = [x.lower() for x in junior]\n",
    "\n",
    "    senior = [\"iv\", \"v\", \"IV\", \"V\", \"iv\",  \"sr\", \"sr.\", \"sre\"]\n",
    "    #senior to lowercase\n",
    "    senior = [x.lower() for x in senior]\n",
    "\n",
    "    prejunior_levels = [\"apprentice\",\"apprenticeship\",\"internship\", \"intern\", \"(Internship)\", \"sdeintern\",\"volunteer\"]\n",
    "    #prejunior_levels to lowercase\n",
    "    prejunior_levels = [x.lower() for x in prejunior_levels]\n",
    "\n",
    "    junior_levels = [\"l 1\", \"t 1\",  \"tier 1\",  \"level 1\", \"(sde1)\", \"entry level\", \"graduate\", \"jr.\", \"junior\", \"l1\", \"level1\", \"sd1\", \"sde2\", \"se1\", \"t1\", \"t1/t2\", \"tier1\"]\n",
    "    #junior_levels to lowercase\n",
    "    junior_levels = [x.lower() for x in junior_levels]\n",
    "\n",
    "    intermediate_levels = [\"l 2\", \"t 2\", \"tier 2\", \"level 2\", \"(sdeii)\", \"ii\", \"ii)\", \"ii,\", \"intermediate\", \"l2\", \"level2\", \"sd2\", \"sde2\", \"t2\", \"tier2\", \"II\", \"ii\"]\n",
    "    #intermediate_levels to lowercase\n",
    "    intermediate_levels = [x.lower() for x in intermediate_levels]\n",
    "    senior_levels = [\"l 3\", \"l 4\", \"l 5\", \"l 6\", \"t 3\", \"t 4\", \"t 5\", \"t 6\", \"t 7\", \"tier 3\", \"tier 4\", \"tier 5\", \"tier 6\", \"tier 7\", \"level 3\", \"level 4\", \"level 5\", \"level 6\", \"level 7\", \"(e3)\", \"(ic3)\", \"(ic4)\", \"(l3)\", \"(l7)\", \"(sde3)\", \"(se2)\", \"e(senior\", \"head\", \"ic5\", \"ict4\", \"ict5\", \"ii/senior\", \"iii\", \"iii)\", \"iii,\", \"III\", \"iii/technical\", \"l3\", \"l4\", \"l5\", \"l6\", \"l7\", \"level4\", \"level5\", \"level6\", \"level7\", \"lead\", \"lead,\", \"level3\", \"principal\", \"principal,\", \"principle\", \"pro/lead\", \"leads\", \"senior\", \"seniors\", \"seniot\", \"senor\", \"sre/devops\", \"srspec\",  \"sr.financial\"]\n",
    "    #senior_levels to lowercase\n",
    "    senior_levels = [x.lower() for x in senior_levels]\n",
    "\n",
    "    management_levels = [\"vice pres\", \"(csa)\", \"assoc\", \"assocaie\", \"associate\", \"associate,\", \"associate1\", \"associateenterprise\", \"associatena\", \"associatw\", \"captain\", \"chef\", \"chief\", \"executiove\", \"executive\", \"executive,\", \"executivre\", 'cso/ciso\"', \"dceo\", \"director\", \"director,\", \"direrctor\", \"management\", \"manager\", \"manager%2c\", \"manager)\", \"manager,\", \"manager/director\", \"manager/engineer\", \"manager/senior\", \"meister/data\", \"officer\", \"president\", \"president;\", \"vp\"]\n",
    "    #management_levels to lowercase\n",
    "    management_levels = [x.lower() for x in management_levels]\n",
    "\n",
    "    rank = \"No Rank\"\n",
    "\n",
    "    job_title_keywords = job_title.split(\" \")\n",
    "\n",
    "    # check if any of the words in job_title_keywords are in prejunior\n",
    "    listed_rank = \"\"\n",
    "    for word in job_title_keywords:\n",
    "        kword = word.strip().lower()\n",
    "        if kword in junior:\n",
    "            rank = \"Junior\"\n",
    "            listed_rank = word.strip()\n",
    "            break\n",
    "        elif kword in senior:\n",
    "            rank = \"Senior\"\n",
    "            listed_rank = word.strip()\n",
    "            break\n",
    "    \n",
    "    if listed_rank == \"\":\n",
    "        for p_levels in prejunior_levels:\n",
    "            if p_levels.lower().strip() in job_title.lower().strip():\n",
    "                rank = \"Prejunior\"\n",
    "                listed_rank = p_levels\n",
    "                break\n",
    "        for j_levels in junior_levels:\n",
    "            if j_levels.lower().strip() in job_title.lower().strip():\n",
    "                rank = \"Junior\"\n",
    "                listed_rank = j_levels\n",
    "                break\n",
    "        for i_levels in intermediate_levels:\n",
    "            if i_levels.lower().strip() in job_title.lower().strip():\n",
    "                rank = \"Intermediate\"\n",
    "                listed_rank = i_levels\n",
    "                break\n",
    "        for s_levels in senior_levels:\n",
    "            if s_levels.lower().strip() in job_title.lower().strip():\n",
    "                rank = \"Senior\"\n",
    "                listed_rank = s_levels\n",
    "                break\n",
    "        for m_levels in management_levels:\n",
    "            if m_levels.lower().strip() in job_title.lower().strip():\n",
    "                rank = \"Management\"\n",
    "                listed_rank = m_levels\n",
    "                break\n",
    "            \n",
    "    return [job_title.replace(listed_rank, \"\").strip(), rank]\n",
    "    \n",
    "# Split job_title into title and rank column\n",
    "df[[\"job_title\", \"job_rank\"]] = (\n",
    "    df[\"job_title\"].apply(extract_job_title_rank).apply(pd.Series)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geonamescache\n",
    "\n",
    "gc = geonamescache.GeonamesCache()\n",
    "cities = pd.DataFrame.from_dict(gc.get_cities(), orient=\"index\")\n",
    "countries = pd.DataFrame.from_dict(gc.get_countries(), orient=\"index\")\n",
    "us_states = pd.DataFrame.from_dict(gc.get_us_states(), orient=\"index\")\n",
    "\n",
    "# location_state column\n",
    "df['location'] = df['location'].str.replace('in ', '') \n",
    "df['location_us_state'] = df['location'].str.split(', ').str[1].dropna().str.upper() \n",
    "df['location_us_state'] = df['location_us_state'].apply(lambda x: x if x in us_states['code'].tolist() else None) \n",
    "df['location'] = df['location'].str.replace(',\\s\\D\\D$', '', regex=True)\n",
    "\n",
    "# location_country column\n",
    "cities = cities.sort_values(by='population') \n",
    "country_abbrev_dict = countries['name'].to_dict() \n",
    "cities['countrycode'] = cities['countrycode'].replace(country_abbrev_dict) \n",
    "cities = cities.set_index('name')\n",
    "city_to_country_dict = cities['countrycode'].to_dict()\n",
    "df['location'] = df['location'].str.split(', ').str[0]\n",
    "df['location_country'] = df['location'].replace(city_to_country_dict)\n",
    "\n",
    "# fixing location_country column and creating a list of countries that are missing\n",
    "indexes_missing = list(df[df['location_country'] == df['location']].index)\n",
    "unique_countries_missing = df['location_country'][indexes_missing].value_counts().index.tolist() ### City name were not known in city_to_country_dict, therefore the city and country are the same\n",
    "dictvalues = []\n",
    "conflicting_cities = []\n",
    "for i in unique_countries_missing: ### Search through prev mentioned list and see if the known city name is an alternative name and append the actual country name to a list. If not, append the city name to a list\n",
    "    try:\n",
    "        gc.search_cities(i)[1]['name']\n",
    "        conflicting_cities.append(i)\n",
    "        dictvalues.append(i)\n",
    "    except IndexError:\n",
    "        try:\n",
    "            dictvalues.append(gc.search_cities(i)[0]['name'])\n",
    "        except IndexError:\n",
    "            dictvalues.append(i)\n",
    "\n",
    "missing_city_to_country_dict = dict(zip(unique_countries_missing,dictvalues)) ### Create a dictionary with the city name as key and the country name as value\n",
    "df['location'] = df['location'].replace(missing_city_to_country_dict)\n",
    "df['location_country'] = df['location'].replace(city_to_country_dict) \n",
    "df.loc[~df['location_country'].isin(countries.name).dropna(), 'location_country'] = None ### Drop all rows where the country is not in the list of countries\n",
    "df.loc[df['location_us_state'].notna(), 'location_country'] = 'United States' ### If the state is known, the country is the US\n",
    "\n",
    "#manual country imputation\n",
    "df.loc[df['location'].isin(['Vāranāsi', 'Ma.Kunnathur', 'North Goa', 'Sri Potti Sriramulu Nellore', 'Bangalore Rural', 'North Twenty Four Parganas', 'Bhetasi Ba Bhag', 'Naugaon' 'Kanpur Nagar', 'Āmer', 'South Tripura', 'Patan-Veraval', 'Abujhati', 'Grant No 11', 'Kanpur Dehat', 'Sant Kabir Nagar', 'Paschim Medinipur', 'Nalbāri', 'Lakshadweep', 'Madhāpur', 'Jangareddigudem', 'Andheri East']), 'location_country'] = 'India'\n",
    "df.loc[df['location'].isin(['Ang Mo Kio New Town', 'Bedok New Town', 'Jurong West New Town', 'Bukit Merah Estate', 'Novena', 'Yishun New Town', 'Tampines New Town', 'Marina South', 'Bukit Batok New Town']), ['location', 'location_country']] = ['Singapore', 'Singapore'] ### Areas in Singapore\n",
    "df.loc[df['location'].isin(['Greenhithe', 'Twycross', 'Bromley', 'Wiggenhall Saint Germans', 'Sydenham', 'Newcastle Upon Tyne', 'Hendon', 'London Colney']), 'location_country'] = 'United Kingdom'\n",
    "df.loc[df['location'].isin(['Al Ḩajarayn', 'Al Mariah United Group', 'Global Village']), ['location', 'location_country']] = ['Dubai', 'United Arab Emirates']\n",
    "df.loc[df['location'].isin(['Kwun Tong', 'Mui Wo Kau Tsuen', 'Causeway Bay']), ['location', 'location_country']] = ['Hong Kong', 'Hong Kong']\n",
    "df.loc[df['location'].isin(['Lachine', 'Chinook Cove', 'Etobicoke', 'Whistler', 'Brisco', 'Bowen Island']), 'location_country'] = 'Canada'\n",
    "df.loc[df['location'].isin(['Torono', 'North Toronto','Midtown Toronto']),['location', 'location_country']] = ['Toronto', 'Canada']\n",
    "df.loc[df['location'].isin(['Yoqne‘Am ‘Illit','Yoqne`Am', 'Yoqne`am', 'Ra`ananna']), ['location','location_country']] = ['Yoqne‘Am ‘Illit', 'Israel']\n",
    "df.loc[df['location'].isin(['North Ryde', 'Chadstone', 'Chermside', 'Youngs Crossing']), 'location_country'] = 'Australia'\n",
    "df.loc[df['location'].isin(['Frankfurt Am Main', 'Grünheide (Mark)', 'Lustadt']), 'location_country'] = 'Germany'\n",
    "df.loc[df['location'].isin(['Portlaoise', 'Monkstown', 'Midleton', 'Dunboyne']), 'location_country'] = 'Ireland'\n",
    "df.loc[df['location'].isin(['Pudong', 'Shanhai']), ['location', 'location_country']] = ['Shanghai', 'China'] ### Area in Shanghai and misspelling.\n",
    "df.loc[df['location'].isin(['Vedbæk', 'Kongens Lyngby', 'Lyngby', 'Gårde']), 'location_country'] = 'Denmark'\n",
    "df.loc[df['location'].isin(['Ciudad De Mexico', 'Oaxaca De Juárez']), 'location_country'] = 'Mexico'\n",
    "df.loc[df['location'].isin(['Herzliyya B', 'Ra`Ananna']), 'location_country'] = 'Israel'\n",
    "\n",
    "df.loc[df['location'] == 'Newyork',      ['location', 'location_us_state', 'location_country']] = ['New York City', 'NY', 'United States'] \n",
    "df.loc[df['location'] == 'SanFrancisco', ['location', 'location_us_state', 'location_country']] = ['San Francisco', 'CA', 'United States']\n",
    "df.loc[df['location'] == 'Laf',          ['location', 'location_us_state', 'location_country']] = ['Lafayette', 'LA', 'United States']\n",
    "\n",
    "df.loc[(df['location'] == 'Pyrmont') & (df['company'] == 'Google'), ['location', 'location_country']] = ['Sydney', 'Australia'] ### Area in Sydney, where Google has a campus.\n",
    "df.loc[(df['location'] == 'Hillsdale') & (df['company'] == 'Apple'), ['location_us_state', 'location_country']] = ['NJ', 'United States'] ### The other person who worked at Hillsdale was from NJ, USA.\n",
    "\n",
    "df.loc[df['location'] == 'Sparrows Point',           ['location_us_state', 'location_country']] = ['MD', 'United States']\n",
    "\n",
    "df.loc[df['location'] == 'Ḩawwārah',                 ['location', 'location_country']] = ['Amman', 'Jordan'] ### Area in Amman\n",
    "df.loc[df['location'] == 'Surat City',               ['location', 'location_country']] = ['Surat', 'India']\n",
    "df.loc[df['location'] == \"St. John'S\",               ['location', 'location_country']] = [\"St. John's\", 'Canada']\n",
    "df.loc[df['location'] == \"Montréal-Ouest\",           ['location', 'location_country']] = [\"Montréal\", 'Canada']\n",
    "df.loc[df['location'] == \"Manil\",                    ['location', 'location_country']] = [\"Manila\", 'Philippines']\n",
    "df.loc[df['location'] == \"Yaba\",                     ['location', 'location_country']] = [\"Lagos\", 'Nigeria']\n",
    "df.loc[df['location'] == 'Newcastle',                ['location', 'location_country']] = ['Newcastle Upon Tyne', 'United Kingdom']\n",
    "df.loc[df['location'] == 'Manchester City Centre',   ['location', 'location_country']] = ['Manchester', 'United Kingdom']\n",
    "df.loc[df['location'] == 'Covent Garden',            ['location', 'location_country']] = ['London', 'United Kingdom'] ### Area in London\n",
    "df.loc[df['location'] == 'Kuala Lumpur City Centre', ['location', 'location_country']] = ['Kuala Lumpur', 'Malaysia']\n",
    "df.loc[df['location'] == 'Knocknaheeny',             ['location', 'location_country']] = ['Cork', 'Ireland']\n",
    "df.loc[df['location'] == 'Pradera Chica',            ['location', 'location_country']] = ['Quito', 'Ecuador']\n",
    "\n",
    "df.loc[df['location'] == 'Reggio Di Calabria',  'location_country'] = 'Italy'\n",
    "df.loc[df['location'] == 'Issy-Les-Moulineaux', 'location_country'] = 'France'\n",
    "df.loc[df['location'] == 'Schiphol',            'location_country'] = 'Netherlands'\n",
    "df.loc[df['location'] == 'Watreso',             'location_country'] = 'Ghana'\n",
    "df.loc[df['location'] == 'Bangloma',            'location_country'] = 'Democratic Republic of the Congo'\n",
    "df.loc[df['location'] == \"Aţ Ţā'If\",            'location_country'] = 'Saudi Arabia'\n",
    "df.loc[df['location'] == \"Qūnah\",               'location_country'] = 'Egypt'\n",
    "df.loc[df['location'] == \"Rajbari\",             'location_country'] = 'Bangladesh'\n",
    "df.loc[df['location'] == 'Haidian',             'location_country'] = 'China'\n",
    "df.loc[df['location'] == 'Jāwā',                'location_country'] = 'Jordan'\n",
    "df.loc[df['location'] == 'Polska',              'location_country'] = 'Poland'\n",
    "df.loc[df['location'] == 'Jabinyānah',          'location_country'] = 'Tunisia'\n",
    "df.loc[df['location'] == 'La Jina',             'location_country'] = 'The Dominican Republic'\n",
    "df.loc[df['location'] == 'Arifiye',             'location_country'] = 'Turkey'\n",
    "df.loc[df['location'] == 'Rostov-Na-Donu',      'location_country'] = 'Russia'\n",
    "df.loc[df['location'] == 'Gujrāt',              'location_country'] = 'Pakistan'\n",
    "\n",
    "#manual conflicting_cities country imputation\n",
    "df.loc[df['location'].isin(['Bān','Ban','Benga', 'ben']), ['location', 'location_country']] = ['Bengaluru', 'India'] ### Ban is another way to spell Bengaluru\n",
    "df.loc[df['location'] == 'Quebec', 'location_country'] = 'Canada' \n",
    "df.loc[df['location'] == 'Belgrad', 'location_country'] = 'Serbia' ### Belgrad is another way to spell Belgrade\n",
    "df.loc[(df['location'] == 'Charlestown') & (df['company'] == 'Apple'), 'location_country'] = 'Australia' ### Apple has a location in Australia called Charlestown, which has an Apple Store\n",
    "df.loc[df['location'] == 'Mirpur', 'location_country'] = 'Pakistan'\n",
    "\n",
    "#manual us states imputation\n",
    "df.loc[df['location'] == 'San Jose', ['location_us_state', 'location_country']] = ['CA', 'United States'] \n",
    "costarica = 'costa|rica|costa rica|rican|costa rican|ricans|costa ricans|asoamazon' ### If the word costa or rica etc. is in the pros, cons or review_title, the country is Costa Rica, otherwise it will later be assigned as San Jose, CA, USA.\n",
    "df.loc[(df['location'] == 'San Jose') & ((df['pros'].str.contains(costarica)) | (df['cons'].str.contains(costarica)) | (df['review_title'].str.contains(costarica))), ['location_us_state', 'location_country']] = [None, 'Costa Rica']\n",
    "\n",
    "df.loc[(df['location']=='Durham') & (df['location_country'] == 'United States'), 'location_us_state'] = 'NC'\n",
    "df.loc[(df['location']=='San Francisco') & (df['location_us_state'].isna()), 'location_us_state'] = 'CA' ### There is one that is from San Francisco, MO, but there are over 1k from CA, so I'll just assume that they are from CA. Otherwise they would probably have specified the state.\n",
    "df.loc[(df['location']=='Limerick') & (df['location_us_state'].isna()), 'location_country'] = 'Ireland' ### Missings are from Apple and Google and they have offices in Limerick, Ireland, also the most populated.\n",
    "df.loc[(df['location']=='Waterford') & (df['location_us_state'].isna() & (df['company'] == 'Apple')), 'location_country'] = 'Ireland' ### Missing are all from Apple and they have offices in Waterford, Ireland, also the most populated.\n",
    "df.loc[(df['location']=='Stratford') & (df['company'] == 'Apple'), 'location_country'] = 'United Kingdom' ### Missing are all from Apple and they have offices in Stratford.\n",
    "df.loc[(df['location']=='Los Angeles') & (df['location_us_state'].isna()), 'location_us_state'] = 'CA' ### There is one that is from San Francisco, MO, but there are over 1k from CA, so I'll just assume that they are from CA. Otherwise they would probably have specified the state.\n",
    "df.loc[(df['location']=='Los Angeles') & (df['company'] == 'Amazon'), 'location_us_state'] = 'CA' ### Missing are all from Amazon and they have offices in London, UK.\n",
    "df.loc[(df['location']=='Eastvale') & (df['company'] == 'Amazon'), 'location_us_state'] = 'CA' ### Missing are all from Amazon and they have offices in Eastvale, CA.\n",
    "df.loc[(df['location']=='Spokane Valley'), 'location_us_state'] = 'WA' ### Only Spokane Valley is in WA.\n",
    "df.loc[(df['location']=='Mountain View') & (df['location_us_state'].isna()), 'location_us_state'] = 'CA' ### 1.2k in total, 99% from CA.\n",
    "df.loc[(df['location']=='Enfield') & (df['location_us_state'].isna()), 'location_country'] = 'United Kingdom' ### London borough\n",
    "df.loc[(df['location']=='University Park') & (df['company'] == 'Amazon') & (df['location_us_state'].isna()), 'location_us_state'] = 'IL' ### Amazon has offices in University Park, IL.\n",
    "df.loc[(df['location']=='Saugus') & (df['company'] == 'Apple') & (df['location_us_state'].isna()), 'location_us_state'] = 'MA' ### Apple has offices close to Saugus, MA.\n",
    "df.loc[(df['location']=='Homewood') & (df['company'] == 'Google') & (df['location_us_state'].isna()), 'location_us_state'] = 'IL' ### Homewood is a Chicago, IL suburb, where Google has offices.\n",
    "df.loc[(df['location']=='Aurora') & (df['company'] == 'Google') & (df['location_us_state'].isna()), 'location_us_state'] = 'IL' ### Aurora is a Chicago, IL suburb, where Google has offices.\n",
    "df.loc[(df['location']=='Salinas') & (df['location_us_state'].isna()), 'location_us_state'] = 'CA' ### Salinas is close to Silicon Valley, CA.\n",
    "df.loc[(df['location']=='Chicago') & (df['location_us_state'].isna()), 'location_us_state'] = 'IL'\n",
    "df.loc[(df['location']=='Goodyear') & (df['location_us_state'].isna()), 'location_us_state'] = 'AZ' ### Goodyear is close to Phoenix, AZ where there are multiple FAANG offices.\n",
    "df.loc[(df['location']=='Worcester') & (df['location_us_state'].isna()), 'location_us_state'] = 'MA' ### All Worcester review in the dataset are from Massachussets.\n",
    "df.loc[(df['location']=='Jurupa Valley') & (df['location_us_state'].isna()), 'location_us_state'] = 'CA' ### All Jurupa Valley review in the dataset are from CA.\n",
    "df.loc[(df['location']=='Burton') & (df['location_us_state'].isna()), 'location_us_state'] = 'IL' ### Highest populated \"Burton\" in USA. And nearby cities work for Microsoft, since they're close to Detroit, MI.\n",
    "\n",
    "df.loc[(df['location_us_state'].isna()) & (df['location_country']=='United States'), 'location_country'] = None ### If we can't specify a state, we can't be sure what country they are from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(pros, cons):\n",
    "    if len(tokenizer.tokenize(str(pros) + str(cons))) <= 512:\n",
    "        return pros + \" \" + cons\n",
    "    while len(tokenizer.tokenize(str(pros) + str(cons))) >= 512:\n",
    "        if len(tokenizer.tokenize(str(pros))) > len(\n",
    "            tokenizer.tokenize(str(cons))\n",
    "        ):  # If pros longer than cons\n",
    "            pros = pros.split(\". \")[:-1]\n",
    "            pros = \". \".join(pros)\n",
    "        if len(tokenizer.tokenize(str(pros))) < len(\n",
    "            tokenizer.tokenize(str(cons))\n",
    "        ) or len(tokenizer.tokenize(str(pros))) == len(\n",
    "            tokenizer.tokenize(str(cons))\n",
    "        ):  # If cons longer than pros\n",
    "            cons = cons.split(\". \")[:-1]\n",
    "            cons = \". \".join(cons)\n",
    "    return pros + \" \" + cons\n",
    "\n",
    "\n",
    "def truncate_single(sentence):\n",
    "    if len(tokenizer.tokenize(str(sentence))) <= 512:\n",
    "        return sentence\n",
    "    while len(tokenizer.tokenize(str(sentence))) >= 512:\n",
    "        sentence = sentence.split(\". \")[:-1]\n",
    "        sentence = \". \".join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pros_and_cons\"] = df.apply(lambda x: truncate(x[\"pros\"], x[\"cons\"]), axis=1)\n",
    "df[\"advice\"] = df[\"advice\"].fillna(\"\")\n",
    "df[\"review_title\"] = df[\"review_title\"].fillna(\"\")\n",
    "df[\"pros_and_cons\"] = df.apply(lambda x: truncate_single(x[\"review_title\"] + \". \" + x[\"pros_and_cons\"] + \" \" + x[\"advice\"]), axis=1)\n",
    "\n",
    "df[\"pros_and_cons\"] = df['pros_and_cons'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentRating = []\n",
    "for i in tqdm(range(0, len(df))):\n",
    "    batch = tokenizer(\n",
    "        df[\"pros_and_cons\"][i : i + 1].tolist(), return_tensors=\"pt\", padding=\"longest\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        sentimentRating.extend(1 + np.argmax(outputs.logits, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['date'], ascending=False)\n",
    "df = df.reset_index(drop=True)\n",
    "df.insert(loc=1, column=\"sentiment\", value=sentimentRating)\n",
    "df['sentiment'] = df['sentiment'].replace({1: 'Negative', 2: 'Neutral', 3: 'Positive'})\n",
    "df['sentiment'] = df['sentiment'].astype('category').str.title()\n",
    "\n",
    "df = df.drop(columns=[\"pros_and_cons\"])\n",
    "df = df.rename(columns={'helpful': 'helpful_count', 'location_country' : 'country', 'location_us_state' : 'us_state', 'location' : 'city'})\n",
    "df = df[['date', 'review_title', 'pros', 'cons', 'advice', 'rating', 'sentiment', 'job_title', 'job_rank', 'company', 'status' ,'experience', 'helpful_count', 'category', 'city', 'us_state', 'country']]\n",
    "\n",
    "df.to_csv('data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('standenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cf943a06f567d7a029a65c28beb80e0c80e1029da2774ef6b5e07cadd91e931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
