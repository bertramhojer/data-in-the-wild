{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from statsmodels.stats import inter_rater as irr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anno(data_dir: str = \"anno/\", verbose: bool = False) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    c = 0\n",
    "    for file in glob(data_dir + \"*.csv\"):\n",
    "        if verbose:\n",
    "            print(f\"Reading in:   {file}\")\n",
    "        test = pd.read_csv(file)\n",
    "        name = file.split(\"/\")[1].split(\".\")[0]\n",
    "        test[\"name\"] = name\n",
    "        dfs.append(test)\n",
    "        c += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Loaded {c} files\")\n",
    "\n",
    "    concat_df = pd.concat(dfs)\n",
    "    return concat_df.reset_index().iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = load_anno()\n",
    "anno[\"sentiment\"] = anno.label.str.split(\"#\")\n",
    "anno[\"sentiment\"].dropna(inplace=True)\n",
    "anno[[\"anno_rating\", \"sentiment\"]] = anno.sentiment.apply(pd.Series)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating\n",
    "IRR fleiss kappa score of annotated rating between annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oliver = anno[anno.name=='oliver'].anno_rating\n",
    "bertram = anno[anno.name=='bertram'].anno_rating\n",
    "bjarke = anno[anno.name=='bjarke'].anno_rating\n",
    "sebbas = anno[anno.name=='sebbas'].anno_rating\n",
    "max = anno[anno.name=='max'].anno_rating\n",
    "ratings = list(zip(oliver, bertram, sebbas, bjarke, max))\n",
    "dats, cats = irr.aggregate_raters(ratings)\n",
    "print(irr.fleiss_kappa(dats, method='fleiss'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment\n",
    "IRR fleiss kappa score of annotated sentiment between annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oliver = anno[anno.name=='oliver'].sentiment\n",
    "bertram = anno[anno.name=='bertram'].sentiment\n",
    "bjarke = anno[anno.name=='bjarke'].sentiment\n",
    "sebbas = anno[anno.name=='sebbas'].sentiment\n",
    "max = anno[anno.name=='max'].sentiment\n",
    "sentiment_rating = list(zip(oliver, bertram, sebbas, bjarke, max))\n",
    "dats, cats = irr.aggregate_raters(sentiment_rating)\n",
    "print(irr.fleiss_kappa(dats, method='fleiss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "review_df = pd.read_feather(\"reviews_dataset.feather\")\n",
    "\n",
    "def load_anno(data_dir: str = \"anno/\", verbose: bool = False) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    c = 0\n",
    "    for file in glob(data_dir + \"*.csv\"):\n",
    "        if verbose:\n",
    "            print(f\"Reading in:   {file}\")\n",
    "        test = pd.read_csv(file)\n",
    "        name = file.split(\"/\")[1].split(\".\")[0]\n",
    "        test[\"name\"] = name\n",
    "        dfs.append(test)\n",
    "        c += 1\n",
    "\n",
    "    if verbose: \n",
    "        print(f\"Loaded {c} files\")\n",
    "\n",
    "    concat_df = pd.concat(dfs)\n",
    "    return concat_df.reset_index().iloc[:, 1:]\n",
    "\n",
    "anno = load_anno()\n",
    "\n",
    "\n",
    "annotation = load_anno()\n",
    "annotation[\"sentiment\"] = annotation.label.str.split(\"#\")\n",
    "annotation[[\"anno_rating\", \"sentiment\"]] = annotation.sentiment.apply(pd.Series)\n",
    "annotation[\"anno_rating\"] = annotation.anno_rating.apply(int)\n",
    "review_df[\"model_sentiment\"] = review_df[\"sentiment\"]\n",
    "review_df.drop(columns=[\"sentiment\"], inplace=True)\n",
    "review_df[\"advice\"].fillna(\"\", inplace=True)\n",
    "review_df[\"text\"] = \"Title: \\n\" + review_df[\"review_title\"] + \"\\n Pros: \\n\" + review_df[\"pros\"] + \"\\n Cons: \\n\" + review_df[\"cons\"] + \"\\n Advice: \\n\" + review_df[\"advice\"]\n",
    "review_df[\"text\"] = [txt.replace(\"\\r\", \"\") for txt in review_df.text]\n",
    "\n",
    "merged_tmp = annotation.merge(review_df, on=[\"text\"]).drop_duplicates()\n",
    "\n",
    "agg_senti = merged_tmp.groupby('text', as_index=False)['sentiment'].agg(pd.Series.mode)\n",
    "agg_senti[\"agg_sentiment\"] = [x[0] if isinstance(x, np.ndarray) else x for x in agg_senti.sentiment]\n",
    "\n",
    "agg_anno = merged_tmp.groupby('text', as_index=False).mean('anno_rating')\n",
    "agg_anno['agg_anno'] = [round(rating) for rating in agg_anno.anno_rating]\n",
    "\n",
    "merged = merged_tmp.merge(agg_anno[['text', 'agg_anno']], on=[\"text\"]).drop_duplicates()\n",
    "merged = merged.merge(agg_senti[['text', 'agg_sentiment']], on=[\"text\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = merged.groupby(\"text\", as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['int_anno_sent'] = [0 if x == \"negative\" else 1 if x == \"neutral\" else 2 for x in tmp.agg_sentiment]\n",
    "tmp['int_model_sent'] = [0 if x == \"negative\" else 1 if x == \"neutral\" else 2 for x in tmp.model_sentiment]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated Rating\n",
    "Aggregated annotator rating vs review-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_rating = tmp.agg_anno.apply(int)\n",
    "rating = tmp.rating.apply(int)\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "print(cohen_kappa_score(anno_rating, rating))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated Sentiment\n",
    "Aggregated annotator sentiment vs model sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_sent = tmp.int_anno_sent.apply(int)\n",
    "model_sent = tmp.int_model_sent.apply(int)\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "print(cohen_kappa_score(anno_sent, model_sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
